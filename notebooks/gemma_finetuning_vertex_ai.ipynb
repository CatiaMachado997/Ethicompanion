{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38a7b19",
   "metadata": {},
   "source": [
    "# EthicCompanion: Gemma 3n Fine-tuning for Hackathon\n",
    "\n",
    "Welcome to the **EthicCompanion Gemma 3n Implementation** for the Google Gemma 3n Impact Challenge! \n",
    "\n",
    "## ðŸŽ¯ Project Overview\n",
    "EthicCompanion leverages **Gemma 3n's multimodal capabilities** to help users navigate information overload with ethical clarity and inner peace.\n",
    "\n",
    "## ðŸš€ Gemma 3n Advantages for EthicCompanion\n",
    "- **Multimodal Input**: Text, image, video, and audio processing\n",
    "- **Efficient Architecture**: 2B/4B effective parameters with selective activation\n",
    "- **Ethical Foundation**: Built with responsible AI principles\n",
    "- **32K Context Window**: Perfect for comprehensive ethical guidance\n",
    "- **140+ Languages**: Global accessibility for ethical support\n",
    "\n",
    "## ðŸ“Š Models We'll Use\n",
    "1. **Gemma 3n E2B** (2B effective parameters) - Fast ethical classifications\n",
    "2. **Gemma 3n E4B** (4B effective parameters) - Complex ethical reasoning\n",
    "3. **Custom Fine-tuned Models** - Specialized for ethical guidance scenarios\n",
    "\n",
    "## ðŸŽ­ Key Features for Hackathon\n",
    "- **Crisis Detection**: Identify users needing immediate support\n",
    "- **Information Overload Management**: Help process overwhelming news\n",
    "- **Ethical Action Recommendations**: Suggest constructive responses\n",
    "- **Multimodal Understanding**: Process text, images, and audio content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be09139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gemma 3n specific requirements for the hackathon\n",
    "!pip install -q kagglehub\n",
    "!pip install -q transformers>=4.53.0  # Gemma 3n requires transformers 4.53.0+\n",
    "!pip install -q timm  # Required for Gemma 3n image processing\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q accelerate\n",
    "!pip install -q datasets\n",
    "!pip install -q peft  # For LoRA fine-tuning\n",
    "!pip install -q bitsandbytes  # For quantization\n",
    "!pip install -q google-cloud-aiplatform\n",
    "!pip install -q google-cloud-storage\n",
    "!pip install -q pillow requests  # For image processing\n",
    "\n",
    "print(\"âœ… Gemma 3n environment setup complete!\")\n",
    "print(\"ðŸ“¦ Key capabilities installed:\")\n",
    "print(\"- Gemma 3n transformers (4.53.0+)\")\n",
    "print(\"- Multimodal processing (text, image, audio)\")\n",
    "print(\"- LoRA fine-tuning capabilities\")\n",
    "print(\"- Google Cloud integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import kagglehub\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Gemma 3n specific imports\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    Gemma3nForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Google Cloud imports\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "\n",
    "# LoRA and fine-tuning imports\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ”§ Using device: {device}\")\n",
    "\n",
    "# Gemma 3n model configurations for hackathon\n",
    "GEMMA_3N_MODELS = {\n",
    "    \"e2b\": {\n",
    "        \"name\": \"google/gemma-3n-e2b\",\n",
    "        \"effective_params\": \"2B\",\n",
    "        \"description\": \"Efficient model for fast ethical classifications\",\n",
    "        \"use_case\": \"Quick content moderation and ethical scope detection\"\n",
    "    },\n",
    "    \"e4b\": {\n",
    "        \"name\": \"google/gemma-3n-e4b\", \n",
    "        \"effective_params\": \"4B\",\n",
    "        \"description\": \"Advanced model for complex ethical reasoning\",\n",
    "        \"use_case\": \"Deep ethical analysis and guidance generation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ðŸ¤– Gemma 3n Models Available:\")\n",
    "for key, model in GEMMA_3N_MODELS.items():\n",
    "    print(f\"  {key.upper()}: {model['name']} ({model['effective_params']} params)\")\n",
    "    print(f\"    Use: {model['use_case']}\")\n",
    "\n",
    "print(\"\\nâœ… Environment ready for Gemma 3n fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0d4b7",
   "metadata": {},
   "source": [
    "## ðŸ”§ Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"project_id\": \"your-project-id\",  # Replace with your GCP project ID\n",
    "    \"region\": \"us-central1\",\n",
    "    \"model_name\": \"google/gemma-2b-it\",  # Base model to fine-tune\n",
    "    \"dataset_path\": \"./ethical_guidance_dataset.jsonl\",\n",
    "    \"output_dir\": \"./ethiccompanion-gemma-finetuned\",\n",
    "    \"wandb_project\": \"ethiccompanion-training\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1\n",
    "}\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=CONFIG[\"project_id\"], location=CONFIG[\"region\"])\n",
    "\n",
    "# Initialize wandb for experiment tracking\n",
    "wandb.init(project=CONFIG[\"wandb_project\"], config=CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed752ab2",
   "metadata": {},
   "source": [
    "## ðŸ“Š Dataset Preparation\n",
    "\n",
    "Let's create a high-quality dataset for ethical guidance training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ethical guidance training dataset\n",
    "def create_ethical_dataset():\n",
    "    \"\"\"\n",
    "    Create a structured dataset for ethical guidance training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample training data - in practice, you'd have much more data\n",
    "    training_conversations = [\n",
    "        {\n",
    "            \"scenario\": \"information_overload\",\n",
    "            \"user_input\": \"I feel overwhelmed by constant news about conflicts. How can I manage this?\",\n",
    "            \"ethical_response\": \"I understand feeling overwhelmed by distressing news is very common and shows your empathy. Here are some ethical approaches to manage information overload: 1) Set specific times for news consumption (like 15 minutes morning and evening) to stay informed without constant exposure. 2) Choose 2-3 trusted, factual news sources rather than consuming from many outlets. 3) Practice the 'constructive consumption' approach - for every piece of distressing news, look for one constructive action you can take or positive story. 4) Use mindfulness techniques like deep breathing when you feel overwhelmed. 5) Remember that staying mentally healthy allows you to help others more effectively. Would you like specific mindfulness techniques or suggestions for constructive actions?\",\n",
    "            \"reasoning\": \"Response acknowledges emotions, provides practical steps, emphasizes balance between staying informed and mental health.\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"ethical_decision\",\n",
    "            \"user_input\": \"I saw disturbing news and want to help, but I don't know how. What are ethical ways to respond?\",\n",
    "            \"ethical_response\": \"Your desire to help shows compassion and ethical awareness. Here are constructive ways to respond to disturbing news: 1) Research reputable organizations working on the issue and consider donating if you're able. 2) Volunteer your time or skills to local organizations addressing similar issues. 3) Share verified, factual information (not unverified content) to raise awareness responsibly. 4) Engage in respectful dialogue with others to promote understanding. 5) Practice 'informed advocacy' - educate yourself deeply before taking action. 6) Support ethical journalism by subscribing to quality news sources. 7) Take care of your mental health so you can sustain your efforts. Remember, even small ethical actions contribute to positive change.\",\n",
    "            \"reasoning\": \"Provides multiple constructive options, emphasizes verification and sustainability of efforts.\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"peace_techniques\",\n",
    "            \"user_input\": \"I need techniques to find inner peace when reading distressing news.\",\n",
    "            \"ethical_response\": \"Finding inner peace while staying engaged with important issues is a valuable skill. Try these evidence-based techniques: 1) **Box Breathing**: Inhale for 4 counts, hold for 4, exhale for 4, hold for 4. Repeat 4-5 times. 2) **5-4-3-2-1 Grounding**: Notice 5 things you see, 4 you can touch, 3 you hear, 2 you smell, 1 you taste. 3) **Compassionate Reframing**: Instead of 'I can't handle this,' try 'This is difficult, and my concern shows I care.' 4) **Constructive Pause**: After reading distressing news, take 5 minutes to think of one positive action before continuing. 5) **Loving-kindness meditation**: Send good wishes to those suffering and to yourself. These techniques help you stay emotionally regulated while maintaining your capacity for empathy and action.\",\n",
    "            \"reasoning\": \"Provides specific, actionable techniques with clear instructions and explains their purpose.\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"crisis_detection\",\n",
    "            \"user_input\": \"I can't stop thinking about global problems and it's affecting my sleep.\",\n",
    "            \"ethical_response\": \"I'm concerned about how this is affecting your well-being. When global issues significantly impact sleep and daily functioning, it's important to take this seriously. Here's what I recommend: 1) **Immediate steps**: Establish a 'news curfew' 2 hours before bedtime, practice relaxation techniques, and limit news consumption to specific times. 2) **Professional support**: Consider speaking with a counselor or therapist who can help you process these feelings and develop coping strategies. 3) **Reframe your concern**: Your care about global issues shows empathy, but you can only help effectively if you're mentally healthy. 4) **Channel worry into action**: Choose one specific way to contribute (volunteering, donating, advocacy) rather than feeling helpless. If sleep problems persist or you feel hopeless, please reach out to a mental health professional. Your well-being matters.\",\n",
    "            \"reasoning\": \"Shows serious concern for user's well-being, provides immediate and long-term strategies, emphasizes professional help when needed.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format for training\n",
    "    formatted_data = []\n",
    "    for conv in training_conversations:\n",
    "        # Create instruction-following format\n",
    "        instruction = f\"\"\"You are EthicCompanion, an AI assistant focused on providing ethical guidance for information management and finding inner peace.\n",
    "\n",
    "User concern: {conv['user_input']}\n",
    "\n",
    "Provide empathetic, ethical guidance that includes:\n",
    "1. Acknowledgment of their feelings\n",
    "2. Practical, actionable steps\n",
    "3. Mindfulness techniques when appropriate\n",
    "4. When to seek professional help if needed\n",
    "\n",
    "Response:\"\"\"\n",
    "        \n",
    "        formatted_data.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": conv['ethical_response'],\n",
    "            \"scenario\": conv['scenario']\n",
    "        })\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Create the dataset\n",
    "training_data = create_ethical_dataset()\n",
    "\n",
    "# Save to file\n",
    "with open(CONFIG[\"dataset_path\"], 'w') as f:\n",
    "    for item in training_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"âœ… Created dataset with {len(training_data)} training examples\")\n",
    "print(f\"ðŸ’¾ Saved to: {CONFIG['dataset_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7611f9",
   "metadata": {},
   "source": [
    "## ðŸ¤– Model Loading and LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with 4-bit quantization for efficient training\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(f\"âœ… Loaded model: {CONFIG['model_name']}\")\n",
    "print(f\"ðŸ”§ Applied LoRA configuration\")\n",
    "print(f\"ðŸ“Š Trainable parameters: {model.get_nb_trainable_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c798ae5",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data Processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the training data\n",
    "    \"\"\"\n",
    "    # Combine instruction and output for causal language modeling\n",
    "    texts = [inst + output for inst, output in zip(examples[\"instruction\"], examples[\"output\"])]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Load and process dataset\n",
    "dataset = load_dataset(\"json\", data_files=CONFIG[\"dataset_path\"])\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split into train/validation\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "\n",
    "print(f\"âœ… Processed {len(train_dataset)} training examples\")\n",
    "print(f\"ðŸ“ Max sequence length: {CONFIG['max_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc68af10",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸â€â™‚ï¸ Training Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8755b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",  # No validation set for this simple example\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"ethiccompanion-gemma-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"ðŸ‹ï¸â€â™‚ï¸ Training configuration ready!\")\n",
    "print(f\"ðŸ“Š Training examples: {len(train_dataset)}\")\n",
    "print(f\"â±ï¸ Estimated training time: {CONFIG['num_epochs']} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "\n",
    "print(\"âœ… Training completed!\")\n",
    "print(f\"ðŸ’¾ Model saved to: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815541e8",
   "metadata": {},
   "source": [
    "## ðŸ§ª Model Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fine_tuned_model(prompt: str, max_length: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Test the fine-tuned model with a prompt\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the input prompt from the response\n",
    "    response = response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test scenarios\n",
    "test_prompts = [\n",
    "    \"\"\"You are EthicCompanion, an AI assistant focused on providing ethical guidance for information management and finding inner peace.\n",
    "\n",
    "User concern: I'm feeling anxious about climate change news. How can I stay informed without feeling hopeless?\n",
    "\n",
    "Provide empathetic, ethical guidance that includes:\n",
    "1. Acknowledgment of their feelings\n",
    "2. Practical, actionable steps\n",
    "3. Mindfulness techniques when appropriate\n",
    "4. When to seek professional help if needed\n",
    "\n",
    "Response:\"\"\",\n",
    "    \n",
    "    \"\"\"You are EthicCompanion, an AI assistant focused on providing ethical guidance for information management and finding inner peace.\n",
    "\n",
    "User concern: How do I know if news is reliable? I'm worried about spreading misinformation.\n",
    "\n",
    "Provide empathetic, ethical guidance that includes:\n",
    "1. Acknowledgment of their feelings\n",
    "2. Practical, actionable steps\n",
    "3. Mindfulness techniques when appropriate\n",
    "4. When to seek professional help if needed\n",
    "\n",
    "Response:\"\"\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing fine-tuned model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n**Test {i}:**\")\n",
    "    print(f\"**Prompt:** {prompt.split('User concern:')[1].split('Provide empathetic')[0].strip()}\")\n",
    "    \n",
    "    response = test_fine_tuned_model(prompt)\n",
    "    print(f\"**Response:** {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26b15e",
   "metadata": {},
   "source": [
    "## ðŸš€ Deployment to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de729c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement deployment to Vertex AI endpoint\n",
    "# This would involve:\n",
    "# 1. Converting the model to a format suitable for Vertex AI\n",
    "# 2. Creating a custom prediction container\n",
    "# 3. Deploying to a Vertex AI endpoint\n",
    "# 4. Testing the deployed endpoint\n",
    "\n",
    "print(\"ðŸš€ Deployment section - TODO\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Convert model to Vertex AI compatible format\")\n",
    "print(\"2. Create custom prediction container\")\n",
    "print(\"3. Deploy to Vertex AI endpoint\")\n",
    "print(\"4. Test deployed endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b8e53",
   "metadata": {},
   "source": [
    "## ðŸ“Š Training Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully fine-tuned a Gemma model for ethical guidance.\n",
    "\n",
    "### What you accomplished:\n",
    "- âœ… Created a structured ethical guidance dataset\n",
    "- âœ… Fine-tuned Gemma using LoRA for parameter efficiency\n",
    "- âœ… Tracked training with Weights & Biases\n",
    "- âœ… Tested the fine-tuned model\n",
    "- âœ… Saved the model for deployment\n",
    "\n",
    "### Next Steps:\n",
    "1. **Expand Dataset**: Create a larger, more diverse training dataset\n",
    "2. **Hyperparameter Tuning**: Experiment with different learning rates, batch sizes\n",
    "3. **Evaluation Metrics**: Implement comprehensive evaluation metrics\n",
    "4. **A/B Testing**: Compare with base models in production\n",
    "5. **Continuous Learning**: Implement feedback loops for model improvement\n",
    "\n",
    "### Integration with EthicCompanion:\n",
    "- Update your `llm_service_enhanced.py` to use this fine-tuned model\n",
    "- Implement model versioning and A/B testing\n",
    "- Monitor performance in production"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
